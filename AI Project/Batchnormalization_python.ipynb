{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-4dcbd946c02b>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\skrtn\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\skrtn\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\skrtn\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\skrtn\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\skrtn\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([784, 256], stddev=0.01))\n",
    "model1 = tf.matmul(X,W1)\n",
    "L1 = tf.nn.relu(model1)\n",
    "L1 = tf.layers.batch_normalization(L1, training= True)\n",
    "W2 = tf.Variable(tf.random_normal([256,256], stddev=0.01))\n",
    "model2 = tf.matmul(L1, W2)\n",
    "L2 = tf.nn.relu(model2)\n",
    "L2 =tf.layers.batch_normalization(L2, training= True)\n",
    "W3 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n",
    "model3 = tf.matmul(L2, W3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model3, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul:0\", shape=(?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "sess2 = tf.Session()\n",
    "batch_xs, batch_ys = mnist.train.next_batch(600)\n",
    "sess.run(model1, feed_dict={X:batch_xs, Y:batch_ys, keep_prob :0.8})\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(L1, feed_dict={X:batch_xs, Y:batch_ys, keep_prob :0.8})\n",
    "myArrayIndex = []\n",
    "myArrayIndex = tf.argmax(L1,1) #각 행별로 가장 큰 인덱스 출력의 결과 리턴\n",
    "myArray = []\n",
    "for i in range(600) : \n",
    "    #myArray.append([L1[i][myArrayIndex[i]],tf.constant(i)])\n",
    "    myArray.append(L1[i][myArrayIndex[i]])\n",
    "    \n",
    "#myArray.sort()\n",
    "#    value = [L1[i][myArrayIndex[i]]]\n",
    "#   index = i\n",
    "#    tempArray = tf.constant(value, index)\n",
    "#    tf.stack([myArray,tempArray])\n",
    "#sess.run(myArrayIndex, feed_dict={X:batch_xs, Y:batch_ys, keep_prob :0.8})\n",
    "#sess.run(L1, feed_dict={X:batch_xs, Y:batch_ys, keep_prob :0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0218148,\n",
       " 2.624965,\n",
       " 3.3480697,\n",
       " 2.755244,\n",
       " 2.4437852,\n",
       " 2.5513258,\n",
       " 2.7989378,\n",
       " 2.6274185,\n",
       " 2.4883075,\n",
       " 3.1945708,\n",
       " 2.949538,\n",
       " 2.2176871,\n",
       " 2.0605626,\n",
       " 2.9077032,\n",
       " 3.4660647,\n",
       " 4.4846973,\n",
       " 2.9042826,\n",
       " 3.0821686,\n",
       " 3.6012983,\n",
       " 2.5677493,\n",
       " 3.5323188,\n",
       " 5.2418666,\n",
       " 2.2792237,\n",
       " 2.8744226,\n",
       " 3.2963681,\n",
       " 2.3908415,\n",
       " 4.5455194,\n",
       " 2.985218,\n",
       " 2.578258,\n",
       " 3.1803946,\n",
       " 2.1296268,\n",
       " 1.6465945,\n",
       " 2.493352,\n",
       " 3.3059454,\n",
       " 3.4608111,\n",
       " 3.1638129,\n",
       " 3.7200274,\n",
       " 2.570765,\n",
       " 2.995031,\n",
       " 2.5702395,\n",
       " 2.4560332,\n",
       " 2.8413758,\n",
       " 2.8751736,\n",
       " 3.827293,\n",
       " 2.4554276,\n",
       " 2.4000745,\n",
       " 2.2035499,\n",
       " 2.6476684,\n",
       " 4.735883,\n",
       " 3.0130785,\n",
       " 3.127102,\n",
       " 3.362946,\n",
       " 2.8677566,\n",
       " 2.65878,\n",
       " 2.250877,\n",
       " 2.0080867,\n",
       " 2.5706377,\n",
       " 2.6667757,\n",
       " 2.0662942,\n",
       " 2.6158304,\n",
       " 3.0603926,\n",
       " 3.4939847,\n",
       " 2.9201365,\n",
       " 2.425886,\n",
       " 4.7341423,\n",
       " 2.892022,\n",
       " 3.2246933,\n",
       " 3.2220283,\n",
       " 3.2176218,\n",
       " 3.2071333,\n",
       " 3.7708175,\n",
       " 2.6604066,\n",
       " 2.587841,\n",
       " 2.2087073,\n",
       " 2.6227348,\n",
       " 3.3387873,\n",
       " 2.680892,\n",
       " 2.904839,\n",
       " 2.142405,\n",
       " 2.9260762,\n",
       " 4.5991364,\n",
       " 3.945655,\n",
       " 1.9168056,\n",
       " 3.2336993,\n",
       " 3.6601105,\n",
       " 2.0214062,\n",
       " 3.7867844,\n",
       " 4.2955785,\n",
       " 3.2823186,\n",
       " 2.3987367,\n",
       " 4.0925837,\n",
       " 2.676103,\n",
       " 2.4775581,\n",
       " 3.1417515,\n",
       " 3.9250054,\n",
       " 2.3803787,\n",
       " 3.0196517,\n",
       " 3.7730863,\n",
       " 3.3904324,\n",
       " 3.2152333,\n",
       " 3.5980713,\n",
       " 3.3514614,\n",
       " 3.4864817,\n",
       " 2.8224843,\n",
       " 2.1488914,\n",
       " 2.554475,\n",
       " 3.36834,\n",
       " 3.5208967,\n",
       " 4.997238,\n",
       " 2.8608694,\n",
       " 3.8562999,\n",
       " 3.243898,\n",
       " 3.2128713,\n",
       " 2.262476,\n",
       " 2.7595742,\n",
       " 2.0192263,\n",
       " 2.4004343,\n",
       " 2.333199,\n",
       " 4.538601,\n",
       " 1.7880781,\n",
       " 3.2202654,\n",
       " 2.3792586,\n",
       " 2.2995892,\n",
       " 1.994514,\n",
       " 3.8225727,\n",
       " 2.809483,\n",
       " 3.6688533,\n",
       " 3.5375419,\n",
       " 2.4173715,\n",
       " 1.9319103,\n",
       " 2.2158186,\n",
       " 2.6363804,\n",
       " 3.4261885,\n",
       " 2.4446337,\n",
       " 3.195129,\n",
       " 2.5761306,\n",
       " 3.688952,\n",
       " 3.5463529,\n",
       " 2.5969982,\n",
       " 3.3257017,\n",
       " 5.1512856,\n",
       " 2.4167976,\n",
       " 3.2021244,\n",
       " 3.94307,\n",
       " 3.4936094,\n",
       " 2.2958488,\n",
       " 2.710975,\n",
       " 2.7666721,\n",
       " 2.5876498,\n",
       " 2.9187968,\n",
       " 3.2810946,\n",
       " 2.150206,\n",
       " 2.925075,\n",
       " 2.498574,\n",
       " 3.235183,\n",
       " 3.2836194,\n",
       " 2.5985384,\n",
       " 2.361808,\n",
       " 3.460548,\n",
       " 2.4120936,\n",
       " 2.8005083,\n",
       " 1.849717,\n",
       " 3.4516263,\n",
       " 2.3004122,\n",
       " 2.4209623,\n",
       " 2.7574217,\n",
       " 2.7758863,\n",
       " 2.7365882,\n",
       " 3.829257,\n",
       " 1.8374798,\n",
       " 3.5982425,\n",
       " 3.003994,\n",
       " 2.502089,\n",
       " 2.9492238,\n",
       " 3.037669,\n",
       " 2.1878555,\n",
       " 2.921143,\n",
       " 4.7362933,\n",
       " 2.6604278,\n",
       " 3.348936,\n",
       " 3.2264907,\n",
       " 3.343046,\n",
       " 4.6149797,\n",
       " 2.361007,\n",
       " 3.1574874,\n",
       " 3.64513,\n",
       " 2.6464043,\n",
       " 2.6686144,\n",
       " 3.1707666,\n",
       " 2.218075,\n",
       " 2.742642,\n",
       " 2.010196,\n",
       " 2.782327,\n",
       " 3.499619,\n",
       " 2.4351542,\n",
       " 2.8105927,\n",
       " 5.4720435,\n",
       " 1.9536436,\n",
       " 2.7281828,\n",
       " 3.7198572,\n",
       " 3.3317034,\n",
       " 2.861702,\n",
       " 2.0470495,\n",
       " 2.8985908,\n",
       " 2.924909,\n",
       " 2.2757356,\n",
       " 2.9671733,\n",
       " 1.8284502,\n",
       " 2.1281097,\n",
       " 3.1775455,\n",
       " 4.4299827,\n",
       " 2.902587,\n",
       " 2.9662948,\n",
       " 2.6036994,\n",
       " 3.265523,\n",
       " 3.5940225,\n",
       " 3.1739023,\n",
       " 2.7783222,\n",
       " 2.573342,\n",
       " 2.0070612,\n",
       " 2.7236738,\n",
       " 3.5359132,\n",
       " 4.624001,\n",
       " 3.404681,\n",
       " 2.8456285,\n",
       " 3.0082426,\n",
       " 2.8379536,\n",
       " 3.8678098,\n",
       " 2.8777366,\n",
       " 2.4920208,\n",
       " 3.572872,\n",
       " 2.372845,\n",
       " 2.7764113,\n",
       " 2.9322598,\n",
       " 3.4440165,\n",
       " 2.2247043,\n",
       " 2.640685,\n",
       " 2.9845881,\n",
       " 2.8597858,\n",
       " 2.1373174,\n",
       " 2.6636105,\n",
       " 3.9549475,\n",
       " 4.5504265,\n",
       " 2.2490878,\n",
       " 2.3406024,\n",
       " 2.017121,\n",
       " 4.5510044,\n",
       " 2.908878,\n",
       " 3.951044,\n",
       " 1.9235201,\n",
       " 3.7208805,\n",
       " 3.3271983,\n",
       " 2.474026,\n",
       " 3.27597,\n",
       " 2.3656738,\n",
       " 2.7323122,\n",
       " 3.1537983,\n",
       " 2.4353507,\n",
       " 2.686463,\n",
       " 3.469264,\n",
       " 2.8652792,\n",
       " 4.7988415,\n",
       " 2.8519945,\n",
       " 4.550943,\n",
       " 2.9761188,\n",
       " 3.0958815,\n",
       " 2.418641,\n",
       " 2.986762,\n",
       " 2.3718169,\n",
       " 4.169321,\n",
       " 2.6329079,\n",
       " 3.0453742,\n",
       " 2.5772362,\n",
       " 3.437928,\n",
       " 3.1604002,\n",
       " 3.7024193,\n",
       " 4.0493846,\n",
       " 2.4413967,\n",
       " 2.3490582,\n",
       " 3.1846485,\n",
       " 3.879997,\n",
       " 3.2660048,\n",
       " 4.596269,\n",
       " 2.7692106,\n",
       " 3.131661,\n",
       " 5.1374187,\n",
       " 3.6515803,\n",
       " 3.3426116,\n",
       " 2.1051502,\n",
       " 4.3195853,\n",
       " 2.3896203,\n",
       " 2.420085,\n",
       " 2.0960526,\n",
       " 5.211756,\n",
       " 4.7061157,\n",
       " 3.2657495,\n",
       " 2.950646,\n",
       " 3.7976637,\n",
       " 2.9769645,\n",
       " 2.3075304,\n",
       " 2.104054,\n",
       " 2.8179717,\n",
       " 2.0634496,\n",
       " 2.3558798,\n",
       " 2.1980605,\n",
       " 3.783957,\n",
       " 2.161487,\n",
       " 2.9537857,\n",
       " 3.087648,\n",
       " 3.7813587,\n",
       " 3.235478,\n",
       " 2.4798052,\n",
       " 2.7815158,\n",
       " 2.3950775,\n",
       " 2.9364462,\n",
       " 3.7572477,\n",
       " 2.9468007,\n",
       " 2.4644752,\n",
       " 2.3853133,\n",
       " 2.5430055,\n",
       " 2.786891,\n",
       " 5.598433,\n",
       " 2.0799782,\n",
       " 2.8422403,\n",
       " 2.7965517,\n",
       " 3.1457,\n",
       " 4.0293913,\n",
       " 2.2164946,\n",
       " 2.4554033,\n",
       " 3.0834298,\n",
       " 3.5910995,\n",
       " 2.6217737,\n",
       " 2.8206592,\n",
       " 2.645641,\n",
       " 3.6330283,\n",
       " 3.8444006,\n",
       " 2.1172311,\n",
       " 3.0522585,\n",
       " 2.6477332,\n",
       " 3.933355,\n",
       " 2.8047607,\n",
       " 2.9667263,\n",
       " 3.4825401,\n",
       " 2.0530372,\n",
       " 2.9540942,\n",
       " 3.491139,\n",
       " 3.0288599,\n",
       " 3.02758,\n",
       " 3.113525,\n",
       " 2.4814215,\n",
       " 2.72142,\n",
       " 1.9229035,\n",
       " 3.5126157,\n",
       " 2.2481537,\n",
       " 2.9000146,\n",
       " 2.8981137,\n",
       " 2.6858149,\n",
       " 2.777861,\n",
       " 4.338215,\n",
       " 3.6524696,\n",
       " 2.7553582,\n",
       " 3.496784,\n",
       " 2.3320003,\n",
       " 2.3558474,\n",
       " 3.1720064,\n",
       " 2.7316709,\n",
       " 3.5892367,\n",
       " 3.2908957,\n",
       " 3.9588256,\n",
       " 3.7044027,\n",
       " 2.822508,\n",
       " 3.5345764,\n",
       " 2.5831063,\n",
       " 4.054882,\n",
       " 3.1177402,\n",
       " 3.1064148,\n",
       " 2.6919413,\n",
       " 2.7063165,\n",
       " 2.2503474,\n",
       " 3.460028,\n",
       " 2.3259838,\n",
       " 2.546228,\n",
       " 2.5593395,\n",
       " 3.3256698,\n",
       " 3.0190396,\n",
       " 2.209286,\n",
       " 3.9354498,\n",
       " 3.0948088,\n",
       " 2.9378843,\n",
       " 3.12205,\n",
       " 2.584914,\n",
       " 4.3220973,\n",
       " 2.882137,\n",
       " 4.2584205,\n",
       " 2.9098604,\n",
       " 2.21826,\n",
       " 1.8543942,\n",
       " 3.8818333,\n",
       " 3.4324903,\n",
       " 3.3887246,\n",
       " 4.067099,\n",
       " 2.3335814,\n",
       " 2.361493,\n",
       " 2.9393685,\n",
       " 2.7777915,\n",
       " 3.2910526,\n",
       " 2.662516,\n",
       " 3.4128356,\n",
       " 2.078852,\n",
       " 2.8522286,\n",
       " 2.5690932,\n",
       " 3.4912055,\n",
       " 3.475309,\n",
       " 3.4280713,\n",
       " 2.4411995,\n",
       " 2.6866782,\n",
       " 4.2253494,\n",
       " 2.9398682,\n",
       " 3.3134832,\n",
       " 2.2227454,\n",
       " 3.5996592,\n",
       " 3.1384404,\n",
       " 2.8047862,\n",
       " 2.4365458,\n",
       " 3.0431104,\n",
       " 3.3271184,\n",
       " 2.180533,\n",
       " 2.3160596,\n",
       " 3.1475766,\n",
       " 2.789717,\n",
       " 2.5647817,\n",
       " 4.0791526,\n",
       " 3.0650954,\n",
       " 2.7009041,\n",
       " 4.412001,\n",
       " 3.289055,\n",
       " 2.8246293,\n",
       " 3.530305,\n",
       " 3.2892733,\n",
       " 3.7154617,\n",
       " 3.296714,\n",
       " 3.6624277,\n",
       " 3.4317513,\n",
       " 2.5654566,\n",
       " 2.126911,\n",
       " 2.9800432,\n",
       " 3.3625374,\n",
       " 3.3372283,\n",
       " 2.5695777,\n",
       " 2.8828576,\n",
       " 2.6002347,\n",
       " 5.2230406,\n",
       " 3.247798,\n",
       " 3.2191854,\n",
       " 2.40805,\n",
       " 3.5049322,\n",
       " 2.5564294,\n",
       " 3.1886036,\n",
       " 3.2495172,\n",
       " 2.4200888,\n",
       " 2.9308972,\n",
       " 4.037036,\n",
       " 2.9382372,\n",
       " 2.6619873,\n",
       " 3.1201472,\n",
       " 2.365282,\n",
       " 1.9819517,\n",
       " 4.647627,\n",
       " 2.4543915,\n",
       " 2.1634595,\n",
       " 2.9855318,\n",
       " 1.9605675,\n",
       " 2.852622,\n",
       " 3.8071337,\n",
       " 2.1287463,\n",
       " 3.021878,\n",
       " 2.7702217,\n",
       " 2.7634726,\n",
       " 3.1917214,\n",
       " 5.1250434,\n",
       " 2.8405626,\n",
       " 3.3105989,\n",
       " 2.4747748,\n",
       " 2.3148322,\n",
       " 3.4316862,\n",
       " 3.3839867,\n",
       " 2.7669985,\n",
       " 2.596723,\n",
       " 2.7370665,\n",
       " 2.4744008,\n",
       " 2.8376095,\n",
       " 4.495856,\n",
       " 2.8369794,\n",
       " 4.7324014,\n",
       " 3.4392357,\n",
       " 2.500621,\n",
       " 3.6964862,\n",
       " 2.4501505,\n",
       " 4.415598,\n",
       " 3.0657725,\n",
       " 2.5735424,\n",
       " 3.3005743,\n",
       " 3.421165,\n",
       " 2.5148904,\n",
       " 3.6065223,\n",
       " 3.877174,\n",
       " 3.855002,\n",
       " 3.695447,\n",
       " 3.5092366,\n",
       " 3.391592,\n",
       " 2.2163684,\n",
       " 2.3479047,\n",
       " 2.1568193,\n",
       " 2.0970318,\n",
       " 3.4303632,\n",
       " 3.5423484,\n",
       " 3.9922314,\n",
       " 2.8386126,\n",
       " 2.4263625,\n",
       " 4.30628,\n",
       " 3.3151617,\n",
       " 3.4239602,\n",
       " 3.356113,\n",
       " 1.90181,\n",
       " 2.7569218,\n",
       " 3.1641212,\n",
       " 2.6544876,\n",
       " 3.734675,\n",
       " 2.7885995,\n",
       " 3.2634056,\n",
       " 3.2683368,\n",
       " 2.4504538,\n",
       " 3.4888248,\n",
       " 4.1797295,\n",
       " 2.6810615,\n",
       " 3.381637,\n",
       " 4.04565,\n",
       " 4.586594,\n",
       " 3.367964,\n",
       " 2.0701165,\n",
       " 3.3195488,\n",
       " 2.7426805,\n",
       " 3.2068436,\n",
       " 2.9843795,\n",
       " 2.8559945,\n",
       " 3.2591572,\n",
       " 3.2064373,\n",
       " 3.349249,\n",
       " 2.7312129,\n",
       " 3.7864823,\n",
       " 2.5172083,\n",
       " 2.8865738,\n",
       " 4.4098067,\n",
       " 2.5624905,\n",
       " 2.1590977,\n",
       " 2.3142195,\n",
       " 2.4249423,\n",
       " 3.0139756,\n",
       " 3.0704367,\n",
       " 2.391229,\n",
       " 3.2189589,\n",
       " 2.0785284,\n",
       " 2.2821488,\n",
       " 4.153479,\n",
       " 3.3490338,\n",
       " 4.5198965,\n",
       " 3.4417624,\n",
       " 3.4813712,\n",
       " 3.5844898,\n",
       " 1.498447,\n",
       " 3.495251,\n",
       " 3.0529172,\n",
       " 2.567613,\n",
       " 5.110264,\n",
       " 2.9299521,\n",
       " 1.6450232,\n",
       " 3.8270054,\n",
       " 3.0781612,\n",
       " 1.9195029,\n",
       " 2.2964728,\n",
       " 1.6850836,\n",
       " 2.492927,\n",
       " 2.7178144,\n",
       " 2.7909317,\n",
       " 3.2286222,\n",
       " 3.2291963,\n",
       " 3.3855624,\n",
       " 2.762598,\n",
       " 1.9157546,\n",
       " 2.9314616,\n",
       " 2.6726615,\n",
       " 3.570469,\n",
       " 3.0133553,\n",
       " 2.8578908,\n",
       " 2.9720693,\n",
       " 3.5582085,\n",
       " 2.3076148,\n",
       " 2.6801498,\n",
       " 3.2447035,\n",
       " 2.888336]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sess.run(L1[0][myArrayIndex[0]], feed_dict={X:batch_xs, Y:batch_ys, keep_prob :0.8}) \n",
    "#sess.run(myArray, feed_dict={X:batch_xs, Y:batch_ys, keep_prob :0.8}) \n",
    "#myArray.sort()\n",
    "sess.run(myArray, feed_dict={X:batch_xs, Y:batch_ys, keep_prob :0.8}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 Avg. cost=  0.412\n",
      "Epoch:  0002 Avg. cost=  0.152\n",
      "Epoch:  0003 Avg. cost=  0.098\n",
      "Epoch:  0004 Avg. cost=  0.071\n",
      "Epoch:  0005 Avg. cost=  0.053\n",
      "Epoch:  0006 Avg. cost=  0.040\n",
      "Epoch:  0007 Avg. cost=  0.031\n",
      "Epoch:  0008 Avg. cost=  0.027\n",
      "Epoch:  0009 Avg. cost=  0.021\n",
      "Epoch:  0010 Avg. cost=  0.017\n",
      "Epoch:  0011 Avg. cost=  0.017\n",
      "Epoch:  0012 Avg. cost=  0.014\n",
      "Epoch:  0013 Avg. cost=  0.012\n",
      "Epoch:  0014 Avg. cost=  0.011\n",
      "Epoch:  0015 Avg. cost=  0.010\n",
      "Epoch:  0016 Avg. cost=  0.009\n",
      "Epoch:  0017 Avg. cost=  0.011\n",
      "Epoch:  0018 Avg. cost=  0.012\n",
      "Epoch:  0019 Avg. cost=  0.010\n",
      "Epoch:  0020 Avg. cost=  0.004\n",
      "Epoch:  0021 Avg. cost=  0.003\n",
      "Epoch:  0022 Avg. cost=  0.015\n",
      "Epoch:  0023 Avg. cost=  0.007\n",
      "Epoch:  0024 Avg. cost=  0.005\n",
      "Epoch:  0025 Avg. cost=  0.005\n",
      "Epoch:  0026 Avg. cost=  0.010\n",
      "Epoch:  0027 Avg. cost=  0.009\n",
      "Epoch:  0028 Avg. cost=  0.004\n",
      "Epoch:  0029 Avg. cost=  0.003\n",
      "Epoch:  0030 Avg. cost=  0.011\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    total_cost = 0\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        #batch_size만큼 mnist데이터 가져오기\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        #실행!\n",
    "        _, cost_val = sess.run([optimizer, cost],\n",
    "                              feed_dict={X:batch_xs, Y:batch_ys, keep_prob :0.8})\n",
    "        \n",
    "        total_cost += cost_val\n",
    "        \n",
    "    print('Epoch: ', '%04d' %(epoch+1), 'Avg. cost= ', '{:.3f}'.format(total_cost/total_batch))\n",
    "    \n",
    "print('최적화 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.9773\n"
     ]
    }
   ],
   "source": [
    "is_correct = tf.equal(tf.argmax(model3,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도:', sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels, keep_prob:1}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
